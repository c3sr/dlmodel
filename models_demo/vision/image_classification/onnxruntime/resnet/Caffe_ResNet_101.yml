name: Caffe_ResNet_101
framework:
  name: Onnxruntime
  version: 1.7.1
version: 1.0
description: >
  This model is a replication of the model described in the Resnet publication originally written in Caffe.
  The pre-trained model expects input in mini-batches of 3-channel BGR images of shape (3 x H x W), where H and W are expected to be 224. The images have to be loaded in to a range of [0, 255] and then normalized using mean = [102.9801, 115.9465, 122.7717] and std = [1, 1, 1].
references:
  - https://github.com/Cadene/pretrained-models.pytorc
  - https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/cafferesnet.py
  - https://github.com/KaimingHe/deep-residual-networks
license: BSD 3-Clause License
modality: image_classification
inputs:
  - type: image
    description: the input image
    parameters:
      element_type: float32
      input_layer: 0
      layout: CHW
      color_mode: RGB
      dimensions: [3, 224, 224]
      mean: [102.9801, 115.9465, 122.7717]
      scale: [1, 1, 1]
outputs:
  - type: classification
    description: the probability
    parameters:
      element_type: float32
model:
  is_archive:
    false
  graph_path: https://s3.amazonaws.com/store.carml.org/models/onnxruntime/cafferesnet101-imagenet.onnx
  graph_checksum: 9f029061ed3b841cb8bf35faa72ba9ef
  features_path: http://s3.amazonaws.com/store.carml.org/synsets/imagenet/synset.txt
  features_checksum: 4d234b5833aca44928065a180db3016a
preprocess: |
  from torchvision import transforms
  from PIL import Image
  preprocessor = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[102.9801 / 255, 115.9465 / 255, 122.7717 / 255], std=[1 / 255, 1 / 255, 1 / 255])
  ])
  def preprocess(ctx, data):
    img = Image.open(dataset[int(data)]).convert('RGB')
    return preprocessor(img).numpy()
postprocess: |
  from scipy.special import softmax
  def postprocess(ctx, data):
    return softmax(data[0], axis = 1).tolist()
attributes:
  kind: CNN
  training_dataset: ImageNet
  manifest_author: Yen-Hsiang Chang
